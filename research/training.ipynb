{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "# from constants import *\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_TEXT = 150\n",
    "MAX_LEN_SUMMARY = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(r\"U:\\nlp_project\\text_summarization\\artifacts\\data\\preprocessed_data\\x_tr.csv\")\n",
    "y_train = pd.read_csv(r\"U:\\nlp_project\\text_summarization\\artifacts\\data\\preprocessed_data\\y_tr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = pd.read_csv(r\"U:\\nlp_project\\text_summarization\\artifacts\\data\\preprocessed_data\\x_val.csv\")\n",
    "y_val = pd.read_csv(r\"U:\\nlp_project\\text_summarization\\artifacts\\data\\preprocessed_data\\y_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train[\"cleaned_text\"]))\n",
    "print(type(y_train[\"cleaned_summary\"]))\n",
    "print(\"\\n\")\n",
    "print(type(x_val[\"cleaned_text\"]))\n",
    "print(type(y_val[\"cleaned_summary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepairing Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_train[\"cleaned_text\"]))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_train    =   x_tokenizer.texts_to_sequences(list(x_train[\"cleaned_text\"])) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(list(x_val[\"cleaned_text\"]))\n",
    "\n",
    "#padding zero upto maximum length\n",
    "# x_train    =   pad_sequences(x_train,  maxlen=MAX_LEN_TEXT, padding='post') \n",
    "# x_val   =   pad_sequences(x_val, maxlen=MAX_LEN_TEXT, padding='post')\n",
    "\n",
    "# x_voc_size   =  len(x_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(l) for l in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(l) for l in x_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_val.shape,x_voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_train[\"cleaned_summary\"]))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_train    =   y_tokenizer.texts_to_sequences(list(y_train[\"cleaned_summary\"])) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(list(y_val[\"cleaned_summary\"])) \n",
    "\n",
    "# #padding zero upto maximum length\n",
    "# y_train    =   pad_sequences(y_train, maxlen=MAX_LEN_SUMMARY, padding='post')\n",
    "# y_val   =   pad_sequences(y_val, maxlen=MAX_LEN_SUMMARY, padding='post')\n",
    "\n",
    "# y_voc_size  =   len(y_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(l) for l in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(l) for l in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape,y_val.shape,y_voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# print(np.isnan(x_train).any(), np.isinf(x_train).any())\n",
    "# print(np.isnan(y_tr).any(), np.isinf(y_tr).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save the tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO SAVE TOKENIZER\n",
    "y_tokenizer,x_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable memory growth for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus[0].device_type:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=1024*2)])\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)  # This will catch the error if it's already initializeda\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Without Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Sequential()\n",
    "# model.add(Embedding(x_voc_size,500,input_length=100))\n",
    "# model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model.predict(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction.shape\n",
    "#o/p: (100, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding(vocab_size,output_dimension,input_len)\n",
    "#o/p:(batch_size,each_seq_len,each_token_is_represented_by_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 500 \n",
    "class create_encoder(keras.Model):\n",
    "    def __init__(self,encoder_inputs,x_voc_size):\n",
    "        super().__init__()\n",
    "        self.encoder_emb = Embedding(x_voc_size, latent_dim,trainable=True,name=\"encoder_emb\")(encoder_inputs)\n",
    "        self.encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,name=\"lstm1\")\n",
    "        self.dropout_one = Dropout((0.2),name=\"dropout_one\")\n",
    "        self.encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,name=\"lstm2\")\n",
    "        self.dropout_two = Dropout((0.2),name=\"dropout_two\")\n",
    "        self.encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,name=\"lstm3\")\n",
    "\n",
    "    def call(self,training):\n",
    "        encoder_output1, state_h1, state_c1 = self.encoder_lstm1(self.encoder_emb)\n",
    "        x = self.dropout_one(encoder_output1, training=training)\n",
    "        encoder_output2, state_h2, state_c2 = self.encoder_lstm2(x)\n",
    "        x = self.dropout_two(encoder_output2, training=training)\n",
    "        encoder_outputs, state_h, state_c = self.encoder_lstm3(x)\n",
    "        return [state_h,state_c]\n",
    "    \n",
    "\n",
    "class create_decoder(keras.Model):\n",
    "    def __init__(self,decoder_inputs):\n",
    "        super().__init__()\n",
    "        self.decoder_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True,name=\"decoder_emb\")(decoder_inputs)\n",
    "        self.decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,name=\"decoder_lstm\") \n",
    "        self.dropout_decoder = Dropout((0.2),name=\"dropout_decoder\")\n",
    "        self.decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "        \n",
    "    def call(self,context_vector,training):\n",
    "        decoder_outputs,decoder_fwd_state, decoder_back_state = self.decoder_lstm(self.decoder_emb_layer,initial_state=[context_vector[0], context_vector[1]])\n",
    "        x = self.dropout_decoder(decoder_outputs, training=training)\n",
    "        decoder_output = self.decoder_dense(x)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = 500 \n",
    "# class create_encoder(keras.Model):\n",
    "#     def __init__(self,encoder_inputs,x_voc_size):\n",
    "#         super().__init__()\n",
    "#         self.encoder_emb = Embedding(x_voc_size, latent_dim,trainable=True,name=\"encoder_emb\")(encoder_inputs)\n",
    "#         self.encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,name=\"lstm1\")\n",
    "#         self.dropout = Dropout((0.2))\n",
    "#         self.encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,name=\"lstm2\")\n",
    "#         self.encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,name=\"lstm3\")\n",
    "\n",
    "#     def call(self,training=False):\n",
    "#         encoder_output1, state_h1, state_c1 = self.encoder_lstm1(self.encoder_emb)\n",
    "#         x = self.dropout(encoder_output1, training=training)\n",
    "#         encoder_output2, state_h2, state_c2 = self.encoder_lstm2(encoder_output1)\n",
    "#         x = self.dropout(encoder_output1, training=training)\n",
    "#         encoder_outputs, state_h, state_c = self.encoder_lstm3(encoder_output2)\n",
    "#         return [state_h,state_c]\n",
    "    \n",
    "\n",
    "# class create_decoder(keras.Model):\n",
    "#     def __init__(self,decoder_inputs):\n",
    "#         super().__init__()\n",
    "#         self.decoder_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True,name=\"decoder_emb\")(decoder_inputs)\n",
    "#         self.decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,name=\"decoder_lstm\") \n",
    "#         self.decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "        \n",
    "#     def call(self,context_vector):\n",
    "#         decoder_outputs,decoder_fwd_state, decoder_back_state = self.decoder_lstm(self.decoder_emb_layer,initial_state=[context_vector[0], context_vector[1]])\n",
    "#         decoder_output = self.decoder_dense(decoder_outputs)\n",
    "#         return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(MAX_LEN_TEXT)\n",
    "# encoder_inputs = Input(shape=(MAX_LEN_TEXT,))\n",
    "encoder = create_encoder(encoder_inputs,x_voc_size)\n",
    "context_vector = encoder.call(True)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "decoder = create_decoder(decoder_inputs)\n",
    "decoder_output = decoder.call(context_vector, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code require graphviz to run\n",
    "from keras.utils import plot_model\n",
    "plot_model(model=model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(moniter=\"val_loss\", factor=0.5,patience=2,minimum_lr='1e-6')\n",
    "callbacks = [es,reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = [[1,2,3,4,5,6],[11,12,13,14,15,16],[21,22,23,24,25,26]]\n",
    "np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_spec = tf.DeviceSpec(job =\"localhost\", replica = 0, device_type = \"GPU\")\n",
    "\n",
    "# Enabling device logging\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    " \n",
    "# Specifying the device\n",
    "with tf.device(device_spec):\n",
    "  # print('Device Spec: ', device_spec.to_string())\n",
    "  history = model.fit([x_train, y_train[:, :-1]], \n",
    "                    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:], \n",
    "                    epochs=100, \n",
    "                    callbacks=callbacks, \n",
    "                    batch_size=50,\n",
    "                    validation_data=([x_val, y_val[:, :-1]], \n",
    "                                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='val') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save tokenizers, and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(r\"U:\\nlp_project\\text_summarization\\artifacts\\model\\ts_model.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tokenizer_dir = os.path.join(r'U:\\nlp_project\\text_summarization\\artifacts\\tokenizer', 'x')\n",
    "# y_tokenizer_dir = os.path.join(r'U:\\nlp_project\\text_summarization\\artifacts\\tokenizer', 'y')\n",
    "# if not os.path.exists(x_tokenizer_dir) and not os.path.exists(x_tokenizer_dir):\n",
    "#     os.mkdir(x_tokenizer_dir)\n",
    "#     os.mkdir(y_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tokenizer_path = os.path.join(x_tokenizer_dir, 'x_tokenizer') + '.pkl'\n",
    "# with open(x_tokenizer_path, 'wb') as file:\n",
    "#     pickle.dump(x_tokenizer, file)\n",
    "\n",
    "# y_tokenizer_path = os.path.join(y_tokenizer_dir, 'y_tokenizer') + '.pkl'\n",
    "# with open(y_tokenizer_path, 'wb') as file:\n",
    "#     pickle.dump(y_tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
